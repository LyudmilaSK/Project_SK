{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"2ND1sji8NEWv","executionInfo":{"status":"ok","timestamp":1671017299162,"user_tz":-300,"elapsed":8370,"user":{"displayName":"Людмила Рябкова","userId":"09418750656736874784"}}},"outputs":[],"source":["import os\n","import shutil\n","import cv2\n","import math\n","import matplotlib.pyplot as plt\n","\n","import numpy as np\n","from scipy import stats\n","from PIL import Image\n","\n","import torch\n","import torchvision\n","from  torchvision import transforms\n","from google.colab.patches import cv2_imshow\n","import itertools"]},{"cell_type":"markdown","metadata":{"id":"A--t5Dq0kWus"},"source":["# Загрузка данных и кадрирование"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22650,"status":"ok","timestamp":1671017321808,"user":{"displayName":"Людмила Рябкова","userId":"09418750656736874784"},"user_tz":-300},"id":"DKas0vzbJ5_g","outputId":"b0f5a838-ffae-4ebf-eda0-cfcb964dbee1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"vKAOBEWiKDdA","executionInfo":{"status":"ok","timestamp":1671017321808,"user_tz":-300,"elapsed":4,"user":{"displayName":"Людмила Рябкова","userId":"09418750656736874784"}}},"outputs":[],"source":["video_model_path = '/content/drive/MyDrive/SF/Project_5/data/coach.mp4'\n","video_person_path = '/content/drive/MyDrive/SF/Project_5/data/student.mp4'"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"zXIa82RQLk-7","executionInfo":{"status":"ok","timestamp":1671017321808,"user_tz":-300,"elapsed":3,"user":{"displayName":"Людмила Рябкова","userId":"09418750656736874784"}}},"outputs":[],"source":["def get_frames(video_file, folder_name):\n","  if not os.path.exists(folder_name):\n","    os.makedirs(folder_name)\n","  count = 0\n","  cap = cv2.VideoCapture(video_file)   # загрузка видео \n","  frameRate = cap.get(5) # частота кадров\n","  while(cap.isOpened()):\n","    frameId = cap.get(1) # номер текущего кадра\n","    ret, frame = cap.read()\n","    if (ret != True):\n","        break\n","    elif (frameId % math.floor(frameRate) == 0):\n","        filename =\"frame%d.jpg\" % count\n","        count+=1\n","        directory = os.path.join(folder_name, filename)\n","        cv2.imwrite(directory, frame)\n","\n","  cap.release()"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"1bUyA_6cLk73","executionInfo":{"status":"ok","timestamp":1671017356121,"user_tz":-300,"elapsed":34316,"user":{"displayName":"Людмила Рябкова","userId":"09418750656736874784"}}},"outputs":[],"source":["get_frames(video_model_path, 'model')\n","get_frames(video_person_path, 'person')"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"VzkFrXH_32f9","executionInfo":{"status":"ok","timestamp":1671017356122,"user_tz":-300,"elapsed":15,"user":{"displayName":"Людмила Рябкова","userId":"09418750656736874784"}}},"outputs":[],"source":["num_foto = sum(os.path.isfile(os.path.join('person', f)) for f in os.listdir('person'))\n","sigmas = np.array([\n","    .26, .25, .25, .35, .35, .79, .79, .72, .72, .62, .62, 1.07, 1.07, .87,\n","    .87, .89, .89]) / 10.0"]},{"cell_type":"markdown","metadata":{"id":"SWl7JkgTzFtF"},"source":["# Загрузка модели и получение предсказания\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["7698f359a10c4684b7edf60eff61dfef","5773d1092cd94bfeab65697dccddae50","1e0aab20c17b4d3a8ce8a34aab977b3f","6bbf2c5c87d14ebb98e7e03960ad0c88","48bf454773664ec789069fcbd13c542b","e83f0dac5262415b88bf93ead68ae9ab","db2a8569dfa64d109ea9dd627e321414","a4be9f35b9b44955a4412ea0cb333190","a85356ca0799433cb09629903e0401cd","95ea2b0d2e6246afa2c5ed12a96cc388","def102ffbf234940a2d8ef65f2e3c4c9"]},"executionInfo":{"elapsed":6717,"status":"ok","timestamp":1671017363224,"user":{"displayName":"Людмила Рябкова","userId":"09418750656736874784"},"user_tz":-300},"id":"E61k1XrppSS2","outputId":"6d795bc9-2097-43cc-e8d4-123689ed67ea"},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/keypointrcnn_resnet50_fpn_coco-fc266e95.pth\" to /root/.cache/torch/hub/checkpoints/keypointrcnn_resnet50_fpn_coco-fc266e95.pth\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0.00/226M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7698f359a10c4684b7edf60eff61dfef"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["KeypointRCNN(\n","  (transform): GeneralizedRCNNTransform(\n","      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","      Resize(min_size=(640, 672, 704, 736, 768, 800), max_size=1333, mode='bilinear')\n","  )\n","  (backbone): BackboneWithFPN(\n","    (body): IntermediateLayerGetter(\n","      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","      (relu): ReLU(inplace=True)\n","      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","      (layer1): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (1): FrozenBatchNorm2d(256, eps=0.0)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer2): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): FrozenBatchNorm2d(512, eps=0.0)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer3): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): FrozenBatchNorm2d(1024, eps=0.0)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (4): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (5): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer4): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): FrozenBatchNorm2d(2048, eps=0.0)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","    )\n","    (fpn): FeaturePyramidNetwork(\n","      (inner_blocks): ModuleList(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (2): Conv2dNormActivation(\n","          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","        (3): Conv2dNormActivation(\n","          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n","        )\n","      )\n","      (layer_blocks): ModuleList(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        )\n","        (1): Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        )\n","        (2): Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        )\n","        (3): Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        )\n","      )\n","      (extra_blocks): LastLevelMaxPool()\n","    )\n","  )\n","  (rpn): RegionProposalNetwork(\n","    (anchor_generator): AnchorGenerator()\n","    (head): RPNHead(\n","      (conv): Sequential(\n","        (0): Conv2dNormActivation(\n","          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","          (1): ReLU(inplace=True)\n","        )\n","      )\n","      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n","      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","  )\n","  (roi_heads): RoIHeads(\n","    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n","    (box_head): TwoMLPHead(\n","      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n","      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n","    )\n","    (box_predictor): FastRCNNPredictor(\n","      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n","      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n","    )\n","    (keypoint_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(14, 14), sampling_ratio=2)\n","    (keypoint_head): KeypointRCNNHeads(\n","      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): ReLU(inplace=True)\n","      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (3): ReLU(inplace=True)\n","      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (5): ReLU(inplace=True)\n","      (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (7): ReLU(inplace=True)\n","      (8): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (9): ReLU(inplace=True)\n","      (10): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (11): ReLU(inplace=True)\n","      (12): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (13): ReLU(inplace=True)\n","      (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (15): ReLU(inplace=True)\n","    )\n","    (keypoint_predictor): KeypointRCNNPredictor(\n","      (kps_score_lowres): ConvTranspose2d(512, 17, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":7}],"source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","weights='KeypointRCNN_ResNet50_FPN_Weights.DEFAULT'\n","model_keypointrcnn = torchvision.models.detection.keypointrcnn_resnet50_fpn(progress=True, weights=weights)\n","model_keypointrcnn.to(device)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"C1-qIO5Dz9QJ","executionInfo":{"status":"ok","timestamp":1671019927563,"user_tz":-300,"elapsed":2564344,"user":{"displayName":"Людмила Рябкова","userId":"09418750656736874784"}}},"outputs":[],"source":["prediction_model = []\n","prediction_person = []\n","\n","transform = transforms.Compose([transforms.ToTensor()]) \n","\n","for foto in range(num_foto):\n","\n","    path_model = '/content/model/frame'+ str(foto)+'.jpg'\n","    path_person = '/content/person/frame'+ str(foto)+'.jpg'\n","\n","    image_model = Image.open(path_model).convert(\"RGB\")\n","    img_tensor_model = transform(image_model)\n","    img_tensor_model = img_tensor_model.unsqueeze(0)\n","\n","    image_person = Image.open(path_person).convert(\"RGB\")\n","    img_tensor_person = transform(image_person)\n","    img_tensor_person = img_tensor_person.unsqueeze(0)\n","\n","    model_keypointrcnn.eval()\n","\n","    img_tensor_model.to(device)\n","    img_tensor_person.to(device)\n","    with torch.no_grad():\n","      prediction_model.append(model_keypointrcnn(img_tensor_model)[0])\n","      prediction_person.append(model_keypointrcnn(img_tensor_person)[0])"]},{"cell_type":"markdown","metadata":{"id":"KXpJRSLF0qJf"},"source":["#Оценка соответствия поз "]},{"cell_type":"code","execution_count":9,"metadata":{"id":"8rooHafj0wTB","executionInfo":{"status":"ok","timestamp":1671019927563,"user_tz":-300,"elapsed":20,"user":{"displayName":"Людмила Рябкова","userId":"09418750656736874784"}}},"outputs":[],"source":["def get_keypoints(output_model, keypoint_threshold=2, conf_threshold=0.9):\n","    all_keypoints = output_model['keypoints']\n","    all_scores = output_model['keypoints_scores']\n","    confs = output_model['scores']\n","    best_keypoints_img = []\n","    bed_keypoints_img = []\n","    # для каждого задетектированного человека\n","    for person_id in range(len(all_keypoints)):\n","        # проверяем степень уверенности детектора\n","        if (confs[person_id] > conf_threshold):\n","            # собираем ключевые точки конкретного человека\n","            keypoints = all_keypoints[person_id, ...]\n","            # собираем скоры для ключевых точек\n","            scores = all_scores[person_id, ...]\n","            # итерируем по каждому скору\n","            keypoints_best_pose = []\n","            bed_keypoints_pose = []\n","            for kp in range(len(scores)):\n","                # запоминаем индексы опорных точек с низкой степенью уверенности\n","                if scores[kp] <= keypoint_threshold:\n","                  bed_keypoints_pose.append(kp)\n","                # конвертируем массив ключевых точек в список целых чисел\n","                keypoint = list(\n","                    map(int, keypoints[kp, :2].detach().numpy())\n","                )\n","                keypoints_best_pose.append(keypoint)\n","            # список ключевых точек с высокой/низкой уверенностью детекции    \n","            best_keypoints_img.append(np.array(keypoints_best_pose))\n","            bed_keypoints_img.append(np.array(bed_keypoints_pose))\n","            \n","    # форматирование списков для работы\n","    bed_keypoints_img = np.array(bed_keypoints_img)       \n","    if bed_keypoints_img.shape[0] == 1:\n","      bed_keypoints_img = bed_keypoints_img.squeeze(0)\n","    \n","    best_keypoints_img = np.array(best_keypoints_img)       \n","    if best_keypoints_img.shape[0] == 1:\n","      best_keypoints_img = best_keypoints_img.squeeze(0)\n","      \n","    return best_keypoints_img, bed_keypoints_img"]},{"cell_type":"code","execution_count":133,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":282,"status":"ok","timestamp":1671025230743,"user":{"displayName":"Людмила Рябкова","userId":"09418750656736874784"},"user_tz":-300},"id":"ulFjem7O2XY8","outputId":"4157f13c-c5f0-43e2-894f-354b04e9a019"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-9-78a038eb5899>:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  bed_keypoints_img = np.array(bed_keypoints_img)\n"]}],"source":["# Соберём два набора ключевых точек\n","model_keypoints = []\n","person_keypoints = []\n","# Индексы сохраненных и неучтённых фото\n","id_pose = []\n","id_cut = []\n","sigmas_cut = []\n","for foto in range(num_foto):\n","    model_poses, idx_pass_model = get_keypoints(prediction_model[foto])\n","    person_poses, idx_pass_person = get_keypoints(prediction_person[foto])\n","    \n","    # отбор фото, где количество достоверно опознанных тренеров равно\n","    # количеству достоверно опознанных учеников\n","    try:\n","      #model_poses[0].ndim == person_poses[0].ndim:\n","      # индексы опорных точек, детерминируемых с низким score у обоих поз\n","      idx_pass = np.concatenate((idx_pass_model,idx_pass_person)).astype(int)\n","\n","      # удаление в обоих наборах опорных точек,\n","      # отсутвующих либо у тренера, либо у ученика\n","      model_poses_cut = np.delete(model_poses, np.unique(idx_pass), axis = 0)\n","      person_poses_cut = np.delete(person_poses, np.unique(idx_pass), axis = 0)\n","    \n","      # удаление констант, связанных с удаляемыми опорными точками\n","      sigmas_cut.append(np.delete(sigmas, np.unique(idx_pass),axis = 0))\n","\n","      # номера фото, участвующих в оценке\n","      id_pose.append(foto)\n","\n","      # два набора ключевых точек для оценки\n","      model_keypoints.append(model_poses_cut)\n","      person_keypoints.append(person_poses_cut)\n","    except:\n","      # номера фото, не участвующих в оценке\n","      # на одной фото найдено больше одной позы, не найдено ни одной позы\n","      id_cut.append(foto)"]},{"cell_type":"code","execution_count":111,"metadata":{"id":"mW2akxyy12nH","executionInfo":{"status":"ok","timestamp":1671024633788,"user_tz":-300,"elapsed":393,"user":{"displayName":"Людмила Рябкова","userId":"09418750656736874784"}}},"outputs":[],"source":["def affine_transformation(person_pose, model_pose):\n","  # С помощью расширенной матрицы можно осуществить умножение вектора x\n","  # на матрицу A и добавление вектора b за счёт единственного матричного умножения.\n","  # Расширенная матрица создаётся путём дополнения векторов \"1\" в конце.\n","  pad = lambda x: np.hstack([x, np.ones((x.shape[0], 1))])\n","  unpad = lambda x: x[:, :-1]\n","  \n","  # Расширим наборы ключевых точек до [[ x y 1] , [x y 1]]\n","  Y = pad(model_pose)\n","  X = pad(person_pose)\n","\n","  # Решим задачу наименьших квадратов X * A = Y\n","  # и найдём матрицу аффинного преобразования A.\n","  A, res, rank, s = np.linalg.lstsq(X, Y)\n","  A[np.abs(A) < 1e-10] = 0  # превратим в \"0\" слишком маленькие значения\n","\n","  # Преобразование входного набора ключевых точек с помощью матрицы А\n","  A_transform = lambda x: unpad(np.dot(pad(x), A))\n","  input_transform = A_transform(person_pose)\n","\n","  return input_transform"]},{"cell_type":"code","execution_count":112,"metadata":{"id":"1T_7sDzvHvRb","executionInfo":{"status":"ok","timestamp":1671024634238,"user_tz":-300,"elapsed":3,"user":{"displayName":"Людмила Рябкова","userId":"09418750656736874784"}}},"outputs":[],"source":["def cos_similarity(pose1, pose2):\n","    # косинусное сходство между всеми ключевыми точками размерностью = 17*17\n","    cossim = pose1.dot(np.transpose(pose2)) / (\n","        np.linalg.norm(pose1, axis=1) * np.linalg.norm(pose2, axis=1)\n","    )\n","    # косинусное сходство между советующими ключевыми точками\n","    cossim_pair = np.diagonal(cossim)\n","\n","    # усредненное косинусное сходство по фигуре\n","    cossim_person = cossim_pair.mean()\n","\n","    return cossim_person"]},{"cell_type":"code","execution_count":113,"metadata":{"id":"y64hJ8vXHxi2","executionInfo":{"status":"ok","timestamp":1671024634853,"user_tz":-300,"elapsed":7,"user":{"displayName":"Людмила Рябкова","userId":"09418750656736874784"}}},"outputs":[],"source":["def compute_oks(sigmas, model_bb, input_keypoints, model_keypoints):\n","    # OKS = exp(-d^2/(2*s^2*k^2))/number_keypoints\n","    \n","    k = 2 * sigmas\n","\n","    # площадь bounding box\n","    s = abs(model_bb[2] - model_bb[0]) * abs(model_bb[3] - model_bb[1])\n","\n","    # расстояние по каждой координате (катеты)\n","    distance = np.subtract(model_keypoints, input_keypoints)\n","\n","    # евклидово расстояние в квадрате\n","    d_square = np.sum(distance**2, axis=1)\n","    \n","    degree = d_square/(2* s**2 * k**2)\n","\n","    e = np.exp(-degree)\n","\n","    return np.mean(e) "]},{"cell_type":"code","execution_count":134,"metadata":{"id":"PU9FEQVTKhXw","executionInfo":{"status":"ok","timestamp":1671025259772,"user_tz":-300,"elapsed":360,"user":{"displayName":"Людмила Рябкова","userId":"09418750656736874784"}}},"outputs":[],"source":["bounding_box = []\n","for foto in id_pose:  \n","  # выбор bounding box у значимой позы\n","  idx_bool = prediction_model[foto]['scores']>0.9\n","  idx_int = idx_bool.nonzero()\n","  idx = int(idx_int)\n","  bb = torch.Tensor.numpy(prediction_model[foto]['boxes'][idx])\n","  bounding_box.append(bb) "]},{"cell_type":"code","execution_count":135,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1671025260248,"user":{"displayName":"Людмила Рябкова","userId":"09418750656736874784"},"user_tz":-300},"id":"OAY-6cXJTgEl","outputId":"bc01b3d9-9d36-4439-8c60-9a48609d6951"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-111-ba605e515d4d>:14: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n","To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n","  A, res, rank, s = np.linalg.lstsq(X, Y)\n"]}],"source":["person_keypoint_aff = []\n","for foto in range(len(id_pose)):\n","  keypoint_aff = affine_transformation(person_keypoints[foto],\n","                                       model_keypoints[foto])\n","  person_keypoint_aff.append(keypoint_aff)"]},{"cell_type":"code","execution_count":136,"metadata":{"id":"vp0qHstYH2BC","executionInfo":{"status":"ok","timestamp":1671025260249,"user_tz":-300,"elapsed":6,"user":{"displayName":"Людмила Рябкова","userId":"09418750656736874784"}}},"outputs":[],"source":["cos_sim = []\n","oks = []\n","cos_sim_print = []\n","oks_print = []\n","for foto in range(len(id_pose)):\n","  cos_sim_pose = cos_similarity(person_keypoint_aff[foto], model_keypoints[foto])\n","  oks_pose = compute_oks(sigmas_cut[foto], bounding_box[foto], person_keypoint_aff[foto], model_keypoints[foto])\n","\n","  cos_sim_pose_print = str(f\"{cos_sim_pose:.0%}\")\n","  oks_pose_print = str(f\"{oks_pose:.0%}\")\n","\n","  cos_sim.append(cos_sim_pose)\n","  oks.append(oks_pose)\n","\n","  cos_sim_print.append(cos_sim_pose_print)\n","  oks_print.append(oks_pose_print)\n"]},{"cell_type":"code","execution_count":137,"metadata":{"id":"GWHpgz3WtrcT","executionInfo":{"status":"ok","timestamp":1671025260594,"user_tz":-300,"elapsed":4,"user":{"displayName":"Людмила Рябкова","userId":"09418750656736874784"}}},"outputs":[],"source":["message = \"error\"\n","for id in id_cut:\n","  cos_sim_print.insert(id, message)\n","  oks_print.insert(id, message)"]},{"cell_type":"markdown","metadata":{"id":"Hg-h8QOQkaQh"},"source":["# Визуализация keypoint и результатов оценки"]},{"cell_type":"code","execution_count":138,"metadata":{"id":"oJLgQjlMzuP7","executionInfo":{"status":"ok","timestamp":1671025263221,"user_tz":-300,"elapsed":290,"user":{"displayName":"Людмила Рябкова","userId":"09418750656736874784"}}},"outputs":[],"source":["def draw_keypoints_per_person(img, all_keypoints, all_scores, confs, thickness, keypoint_threshold=2, conf_threshold=0.9):\n","    # создаём спектр цветов\n","    cmap = list(itertools.permutations((200, 100, 0), 3))\n","    # создаём копию изображений\n","    img_copy = img.copy()\n","    # для каждого задетектированного человека\n","    for person_id in range(len(all_keypoints)):\n","        # проверяем степень уверенности детектора\n","        if confs[person_id] > conf_threshold:\n","            # собираем ключевые точки конкретного человека\n","            keypoints = all_keypoints[person_id, ...]\n","            # собираем скоры для ключевых точек\n","            scores = all_scores[person_id, ...]\n","            # итерируем по каждому скору\n","            for kp in range(len(scores)):\n","                # проверяем степень уверенности детектора опорной точки\n","                if scores[kp] > keypoint_threshold:\n","                    # конвертируем массив ключевых точек в список целых чисел\n","                    keypoint = tuple(\n","                        map(int, keypoints[kp, :2].detach().numpy().tolist())\n","                    )\n","                    # рисуем круг радиуса thickness вокруг точки\n","                    cv2.circle(img_copy, keypoint, thickness, cmap[person_id], -1)\n","\n","    return img_copy"]},{"cell_type":"code","execution_count":139,"metadata":{"id":"3qfxTJ5rzqik","executionInfo":{"status":"ok","timestamp":1671025263665,"user_tz":-300,"elapsed":3,"user":{"displayName":"Людмила Рябкова","userId":"09418750656736874784"}}},"outputs":[],"source":["def get_limbs_from_keypoints(keypoints):\n","    limbs = [\n","        [keypoints.index(\"right_eye\"), keypoints.index(\"nose\")],\n","        [keypoints.index(\"right_eye\"), keypoints.index(\"right_ear\")],\n","        [keypoints.index(\"left_eye\"), keypoints.index(\"nose\")],\n","        [keypoints.index(\"left_eye\"), keypoints.index(\"left_ear\")],\n","        [keypoints.index(\"right_shoulder\"), keypoints.index(\"right_elbow\")],\n","        [keypoints.index(\"right_elbow\"), keypoints.index(\"right_wrist\")],\n","        [keypoints.index(\"left_shoulder\"), keypoints.index(\"left_elbow\")],\n","        [keypoints.index(\"left_elbow\"), keypoints.index(\"left_wrist\")],\n","        [keypoints.index(\"right_hip\"), keypoints.index(\"right_knee\")],\n","        [keypoints.index(\"right_knee\"), keypoints.index(\"right_ankle\")],\n","        [keypoints.index(\"left_hip\"), keypoints.index(\"left_knee\")],\n","        [keypoints.index(\"left_knee\"), keypoints.index(\"left_ankle\")],\n","        [keypoints.index(\"right_shoulder\"), keypoints.index(\"left_shoulder\")],\n","        [keypoints.index(\"right_hip\"), keypoints.index(\"left_hip\")],\n","        [keypoints.index(\"right_shoulder\"), keypoints.index(\"right_hip\")],\n","        [keypoints.index(\"left_shoulder\"), keypoints.index(\"left_hip\")],\n","    ]\n","    return limbs"]},{"cell_type":"code","execution_count":140,"metadata":{"id":"4G6EdI9YzmuS","executionInfo":{"status":"ok","timestamp":1671025264324,"user_tz":-300,"elapsed":3,"user":{"displayName":"Людмила Рябкова","userId":"09418750656736874784"}}},"outputs":[],"source":["keypoints = ['nose','left_eye','right_eye',\\\n","             'left_ear','right_ear','left_shoulder',\\\n","             'right_shoulder','left_elbow','right_elbow',\\\n","             'left_wrist','right_wrist','left_hip',\\\n","             'right_hip','left_knee', 'right_knee', \\\n","             'left_ankle','right_ankle']"]},{"cell_type":"code","execution_count":141,"metadata":{"id":"je-nfAdGzgkC","executionInfo":{"status":"ok","timestamp":1671025264566,"user_tz":-300,"elapsed":3,"user":{"displayName":"Людмила Рябкова","userId":"09418750656736874784"}}},"outputs":[],"source":["limbs = get_limbs_from_keypoints(keypoints)"]},{"cell_type":"code","execution_count":142,"metadata":{"id":"No1RfUDfzf82","executionInfo":{"status":"ok","timestamp":1671025265303,"user_tz":-300,"elapsed":6,"user":{"displayName":"Людмила Рябкова","userId":"09418750656736874784"}}},"outputs":[],"source":["def draw_skeleton_per_person(img, all_keypoints, all_scores, confs, thickness, keypoint_threshold=2, conf_threshold=0.9):\n","\n","    # создаём спектр цветов\n","    cmap = list(itertools.permutations((200, 100, 0), 3))\n","    # создаём копию изображений\n","    img_copy = img.copy()\n","    # если keypoints детектированы\n","    if len(all_keypoints)>0:\n","        # для каждого задетектированного человека\n","        for person_id in range(len(all_keypoints)):\n","            # проверяем степень уверенности детектора\n","            if confs[person_id]>conf_threshold:\n","            # собираем ключевые точки конкретного человека\n","                keypoints = all_keypoints[person_id, ...]\n","\n","                # для каждой конечности\n","                for limb_id in range(len(limbs)):\n","                    # отмечаем начало конечности\n","                    limb_loc1 = keypoints[limbs[limb_id][0], :2].detach().numpy().astype(np.int32)\n","                    # отмечаем окончание конечности\n","                    limb_loc2 = keypoints[limbs[limb_id][1], :2].detach().numpy().astype(np.int32)\n","                    # определяем скор по конечности как минимальный скор среди ключевых точек конечности\n","                    limb_score = min(all_scores[person_id, limbs[limb_id][0]], all_scores[person_id, limbs[limb_id][1]])\n","                    # проверяем степень уверенности детектора опорной точки\n","                    if limb_score> keypoint_threshold:\n","                        # рисуем линии вдоль конечности\n","                        cv2.line(img_copy, tuple(limb_loc1), tuple(limb_loc2), cmap[person_id], thickness)\n","\n","    return img_copy"]},{"cell_type":"code","execution_count":143,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1rAm2f_b3w57Z_nO3zxOlPzT-YK4CYEG9"},"id":"Tx8iG-BYeyyI","outputId":"521e61e7-9099-441f-e583-a66bd0e40668","executionInfo":{"status":"ok","timestamp":1671025309346,"user_tz":-300,"elapsed":43091,"user":{"displayName":"Людмила Рябкова","userId":"09418750656736874784"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["step_imshow = 3\n","step_save = 6\n","nrows = len(list(range(0, num_foto, step_imshow)))\n","\n","figure, ax = plt.subplots(nrows=nrows, ncols=2, figsize=(15, 6*nrows))\n","\n","# визуализация каждой третьей секунды обоих видео\n","for num_rows, foto in zip(range(nrows),range(0, num_foto, step_imshow)):\n","    path_model = '/content/model/frame'+ str(foto)+'.jpg'\n","    path_person = '/content/person/frame'+ str(foto)+'.jpg'\n","\n","    img_arr_model = cv2.imread(path_model)\n","    img_arr_model = cv2.cvtColor(img_arr_model, cv2.COLOR_BGR2RGB)\n","\n","    img_arr_person = cv2.imread(path_person)\n","    img_arr_person = cv2.cvtColor(img_arr_person, cv2.COLOR_BGR2RGB)\n","\n","    # коуч с опорными точками\n","    model_with_point = draw_keypoints_per_person(\n","        img_arr_model,\n","        all_keypoints = prediction_model[foto]['keypoints'],\n","        all_scores = prediction_model[foto]['keypoints_scores'],\n","        confs = prediction_model[foto]['scores'],\n","        thickness = 5\n","        )\n","    \n","    # коуч с опорными точками и каркасом\n","    model_with_skeleton = draw_skeleton_per_person(\n","        model_with_point,\n","        all_keypoints = prediction_model[foto]['keypoints'],\n","        all_scores = prediction_model[foto]['keypoints_scores'],\n","        confs = prediction_model[foto]['scores'],\n","        thickness = 3\n","        )\n","    \n","    # студент с опорными точками\n","    person_with_point = draw_keypoints_per_person(\n","        img_arr_person,\n","        all_keypoints = prediction_person[foto]['keypoints'],\n","        all_scores = prediction_person[foto]['keypoints_scores'],\n","        confs = prediction_person[foto]['scores'],\n","        thickness = 4\n","        )\n","    \n","    # студент с опорными точками и каркасом\n","    person_with_skeleton = draw_skeleton_per_person(\n","        person_with_point,\n","        all_keypoints = prediction_person[foto]['keypoints'],\n","        all_scores = prediction_person[foto]['keypoints_scores'],\n","        confs = prediction_person[foto]['scores'],\n","        thickness = 2\n","        )\n","    \n","    # вывод данных\n","    ax[num_rows, 0].imshow(model_with_skeleton)\n","    ax[num_rows, 1].imshow(person_with_skeleton)\n","    \n","    ax[num_rows, 0].set_title(\"Сoach\")\n","    ax[num_rows, 1].set_title(''.join(['Person. Cosine similarity ',\n","                                   cos_sim_print[foto],\n","                                   ' OKS ',\n","                                   oks_print[foto]]))\n","    ax[num_rows, 0].set_axis_off()\n","    ax[num_rows, 1].set_axis_off()\n","\n","    coach_bgr = cv2.cvtColor(model_with_skeleton, cv2.COLOR_RGB2BGR)\n","    person_bgr = cv2.cvtColor(person_with_skeleton, cv2.COLOR_RGB2BGR)\n","    \n","    # нанесение метрик на отдельные кадры студента\n","    # и их сохранение вместе с кадром из видео коуча\n","    if foto % step_save == 0:\n","      # нанесение метрик на фото студента\n","      font = cv2.FONT_HERSHEY_SIMPLEX\n","      cv2.putText(person_bgr,\n","                  ''.join(['Limb direction ', cos_sim_print[foto]]),\n","                  (50, 50), \n","                  font, 0.5, \n","                  (255, 0, 0), \n","                  1, \n","                  cv2.LINE_4\n","                  )\n","      \n","      cv2.putText(person_bgr,\n","                  ''.join([' Joint position ', oks_print[foto]]),\n","                  (50, 75), \n","                  font, 0.5, \n","                  (255, 0, 0), \n","                  1, \n","                  cv2.LINE_4\n","                  )\n","      # изменение размера изображения коуча\n","      new_size = person_bgr.shape[:-1][::-1]\n","      resized_coach = cv2.resize(coach_bgr, new_size,\n","                                 interpolation = cv2.INTER_AREA)\n","     # сохранение файла\n","      folder_name = '/content/tandem/' \n","      if not os.path.exists(folder_name):\n","        os.makedirs(folder_name)\n","      filename =\"coach_person_%d.jpg\" % foto\n","      directory = os.path.join(folder_name, filename)\n","      tandem_img = np.concatenate((resized_coach, person_bgr),\n","                                  axis=1)\n","      cv2.imwrite(directory, tandem_img)\n","  \n","    \n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","source":["# Нанесение метрик на видео"],"metadata":{"id":"K-TcAuxxD2gB"}},{"cell_type":"code","execution_count":144,"metadata":{"id":"8f0rWBtIRczs","executionInfo":{"status":"ok","timestamp":1671025309379,"user_tz":-300,"elapsed":59,"user":{"displayName":"Людмила Рябкова","userId":"09418750656736874784"}}},"outputs":[],"source":["# создание видео из всех изображений студента\n","cap = cv2.VideoCapture(video_person_path)  \n","  \n","# Определение кодека и создание видеозаписи\n","\n","frame_width = int(cap.get(3))\n","frame_height = int(cap.get(4))\n","   \n","size = (frame_width, frame_height)\n","\n","fourcc = cv2.VideoWriter_fourcc(*'MPEG')\n","out = cv2.VideoWriter('output_video.mp4', fourcc, 2.0, size)\n","id_foto = 0\n","\n","while id_foto != num_foto:\n","    ret, frame = cap.read() \n","    if(ret):\n","      path = '/content/person/frame'+ str(id_foto)+'.jpg'\n","      img_arr = cv2.imread(path)\n","      prediction = prediction_person[id_foto]\n","\n","      # студент с опорными точками\n","      person_with_point = draw_keypoints_per_person(\n","          img_arr,\n","          all_keypoints = prediction['keypoints'],\n","          all_scores = prediction['keypoints_scores'],\n","          confs = prediction['scores'],\n","          thickness = 4\n","          )\n","      \n","      # студент с опорными точками и каркасом\n","      person_with_skeleton = draw_skeleton_per_person(\n","          person_with_point,\n","          all_keypoints = prediction['keypoints'],\n","          all_scores = prediction['keypoints_scores'],\n","          confs = prediction['scores'],\n","          thickness = 2\n","          )\n","      \n","      # нанесение метрик на фото\n","      font = cv2.FONT_HERSHEY_SIMPLEX\n","      cv2.putText(person_with_skeleton,\n","                  ''.join(['Limb direction ', cos_sim_print[id_foto]]),\n","                  (50, 50), \n","                  font, 0.5, \n","                  (255, 0, 0), \n","                  1, \n","                  cv2.LINE_4\n","                  )\n","      \n","      cv2.putText(person_with_skeleton,\n","                  ''.join([' Joint position ', oks_print[id_foto]]),\n","                  (50, 75), \n","                  font, 0.5, \n","                  (255, 0, 0), \n","                  1, \n","                  cv2.LINE_4\n","                  )\n","      # запись фрема\n","      out.write(person_with_skeleton)      \n","      id_foto+=1\n","      #cv2_imshow(person_with_skeleton)\n","        \n","      # ключ для выхода из записи\n","      if cv2.waitKey(1) & 0xFF == ord('a'):\n","          break\n","  \n","cap.release()\n","out.release() \n","cv2.destroyAllWindows()"]},{"cell_type":"code","source":["# количество ключевых точек на всех фотографиях\n","num_kpoints_person = 17*len(id_pose)\n","# количество значимых ключевых точек\n","num_cut_kpoints = 0\n","for foto in range(len(id_pose)):\n","  num_cut_kpoints += person_keypoints[foto].shape[0]"],"metadata":{"id":"oCWOkkCd8tHM","executionInfo":{"status":"ok","timestamp":1671025309379,"user_tz":-300,"elapsed":48,"user":{"displayName":"Людмила Рябкова","userId":"09418750656736874784"}}},"execution_count":145,"outputs":[]},{"cell_type":"code","source":["print(f'Average limb direction = {(stats.mode(cos_sim)[0].item()):.2%}')\n","print(f'Average joint position = {(stats.mode(oks)[0].item()):.2%}')\n","print(f'{(len(id_cut)/num_foto):.0%} images were unvalued')\n","print(f'{(1 - num_cut_kpoints/num_kpoints_person):.0%} key points on valued images were deleted')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lwDvl-E85VQV","executionInfo":{"status":"ok","timestamp":1671025309389,"user_tz":-300,"elapsed":57,"user":{"displayName":"Людмила Рябкова","userId":"09418750656736874784"}},"outputId":"3380a99e-5c8d-4abe-f346-b8b4df57c560"},"execution_count":146,"outputs":[{"output_type":"stream","name":"stdout","text":["Average limb direction = 98.86%\n","Average joint position = 100.00%\n","7% images were unvalued\n","15% key points on valued images were deleted\n"]}]},{"cell_type":"code","source":["!zip -r tandem.zip tandem/ "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SieU8Ve6t0lG","executionInfo":{"status":"ok","timestamp":1671019972182,"user_tz":-300,"elapsed":56,"user":{"displayName":"Людмила Рябкова","userId":"09418750656736874784"}},"outputId":"0150dba8-7707-4227-96c9-c89877f0a473"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["  adding: tandem/ (stored 0%)\n","  adding: tandem/coach_person_36.jpg (deflated 0%)\n","  adding: tandem/coach_person_30.jpg (deflated 0%)\n","  adding: tandem/coach_person_108.jpg (deflated 0%)\n","  adding: tandem/coach_person_84.jpg (deflated 0%)\n","  adding: tandem/coach_person_66.jpg (deflated 0%)\n","  adding: tandem/coach_person_24.jpg (deflated 0%)\n","  adding: tandem/coach_person_6.jpg (deflated 0%)\n","  adding: tandem/coach_person_138.jpg (deflated 0%)\n","  adding: tandem/coach_person_0.jpg (deflated 0%)\n","  adding: tandem/coach_person_114.jpg (deflated 0%)\n","  adding: tandem/coach_person_96.jpg (deflated 0%)\n","  adding: tandem/coach_person_132.jpg (deflated 0%)\n","  adding: tandem/coach_person_78.jpg (deflated 0%)\n","  adding: tandem/coach_person_54.jpg (deflated 0%)\n","  adding: tandem/coach_person_12.jpg (deflated 0%)\n","  adding: tandem/coach_person_60.jpg (deflated 0%)\n","  adding: tandem/coach_person_102.jpg (deflated 0%)\n","  adding: tandem/coach_person_90.jpg (deflated 0%)\n","  adding: tandem/coach_person_42.jpg (deflated 0%)\n","  adding: tandem/coach_person_48.jpg (deflated 0%)\n","  adding: tandem/coach_person_144.jpg (deflated 0%)\n","  adding: tandem/coach_person_72.jpg (deflated 0%)\n","  adding: tandem/coach_person_126.jpg (deflated 0%)\n","  adding: tandem/coach_person_120.jpg (deflated 0%)\n","  adding: tandem/coach_person_18.jpg (deflated 0%)\n"]}]},{"cell_type":"code","source":["!zip -r mp4tojpg_model.zip model/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gmBtVvjgmqsU","executionInfo":{"status":"ok","timestamp":1671019972182,"user_tz":-300,"elapsed":52,"user":{"displayName":"Людмила Рябкова","userId":"09418750656736874784"}},"outputId":"7147769b-31e9-4d01-c68f-2435e5689a54"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["  adding: model/ (stored 0%)\n","  adding: model/frame139.jpg (deflated 7%)\n","  adding: model/frame67.jpg (deflated 6%)\n","  adding: model/frame129.jpg (deflated 6%)\n","  adding: model/frame97.jpg (deflated 5%)\n","  adding: model/frame36.jpg (deflated 6%)\n","  adding: model/frame69.jpg (deflated 6%)\n","  adding: model/frame76.jpg (deflated 6%)\n","  adding: model/frame100.jpg (deflated 6%)\n","  adding: model/frame12.jpg (deflated 5%)\n","  adding: model/frame5.jpg (deflated 5%)\n","  adding: model/frame104.jpg (deflated 5%)\n","  adding: model/frame145.jpg (deflated 5%)\n","  adding: model/frame122.jpg (deflated 5%)\n","  adding: model/frame120.jpg (deflated 7%)\n","  adding: model/frame82.jpg (deflated 4%)\n","  adding: model/frame45.jpg (deflated 5%)\n","  adding: model/frame1.jpg (deflated 6%)\n","  adding: model/frame6.jpg (deflated 5%)\n","  adding: model/frame54.jpg (deflated 5%)\n","  adding: model/frame83.jpg (deflated 6%)\n","  adding: model/frame42.jpg (deflated 5%)\n","  adding: model/frame51.jpg (deflated 7%)\n","  adding: model/frame134.jpg (deflated 7%)\n","  adding: model/frame34.jpg (deflated 6%)\n","  adding: model/frame127.jpg (deflated 5%)\n","  adding: model/frame23.jpg (deflated 5%)\n","  adding: model/frame78.jpg (deflated 5%)\n","  adding: model/frame92.jpg (deflated 6%)\n","  adding: model/frame2.jpg (deflated 5%)\n","  adding: model/frame37.jpg (deflated 5%)\n","  adding: model/frame109.jpg (deflated 7%)\n","  adding: model/frame41.jpg (deflated 6%)\n","  adding: model/frame0.jpg (deflated 7%)\n","  adding: model/frame52.jpg (deflated 6%)\n","  adding: model/frame49.jpg (deflated 6%)\n","  adding: model/frame43.jpg (deflated 5%)\n","  adding: model/frame148.jpg (deflated 6%)\n","  adding: model/frame75.jpg (deflated 7%)\n","  adding: model/frame22.jpg (deflated 5%)\n","  adding: model/frame63.jpg (deflated 6%)\n","  adding: model/frame141.jpg (deflated 7%)\n","  adding: model/frame57.jpg (deflated 4%)\n","  adding: model/frame119.jpg (deflated 7%)\n","  adding: model/frame116.jpg (deflated 6%)\n","  adding: model/frame62.jpg (deflated 6%)\n","  adding: model/frame68.jpg (deflated 6%)\n","  adding: model/frame70.jpg (deflated 6%)\n","  adding: model/frame135.jpg (deflated 6%)\n","  adding: model/frame103.jpg (deflated 5%)\n","  adding: model/frame96.jpg (deflated 5%)\n","  adding: model/frame85.jpg (deflated 7%)\n","  adding: model/frame7.jpg (deflated 5%)\n","  adding: model/frame142.jpg (deflated 7%)\n","  adding: model/frame115.jpg (deflated 6%)\n","  adding: model/frame59.jpg (deflated 6%)\n","  adding: model/frame29.jpg (deflated 5%)\n","  adding: model/frame107.jpg (deflated 6%)\n","  adding: model/frame79.jpg (deflated 5%)\n","  adding: model/frame126.jpg (deflated 5%)\n","  adding: model/frame31.jpg (deflated 6%)\n","  adding: model/frame77.jpg (deflated 6%)\n","  adding: model/frame117.jpg (deflated 6%)\n","  adding: model/frame20.jpg (deflated 5%)\n","  adding: model/frame146.jpg (deflated 5%)\n","  adding: model/frame88.jpg (deflated 6%)\n","  adding: model/frame32.jpg (deflated 5%)\n","  adding: model/frame128.jpg (deflated 5%)\n","  adding: model/frame108.jpg (deflated 5%)\n","  adding: model/frame28.jpg (deflated 5%)\n","  adding: model/frame140.jpg (deflated 8%)\n","  adding: model/frame24.jpg (deflated 5%)\n","  adding: model/frame106.jpg (deflated 6%)\n","  adding: model/frame80.jpg (deflated 5%)\n","  adding: model/frame138.jpg (deflated 7%)\n","  adding: model/frame66.jpg (deflated 6%)\n","  adding: model/frame47.jpg (deflated 7%)\n","  adding: model/frame93.jpg (deflated 6%)\n","  adding: model/frame71.jpg (deflated 6%)\n","  adding: model/frame81.jpg (deflated 5%)\n","  adding: model/frame55.jpg (deflated 5%)\n","  adding: model/frame44.jpg (deflated 5%)\n","  adding: model/frame73.jpg (deflated 6%)\n","  adding: model/frame25.jpg (deflated 5%)\n","  adding: model/frame58.jpg (deflated 6%)\n","  adding: model/frame95.jpg (deflated 6%)\n","  adding: model/frame38.jpg (deflated 6%)\n","  adding: model/frame94.jpg (deflated 6%)\n","  adding: model/frame72.jpg (deflated 6%)\n","  adding: model/frame90.jpg (deflated 6%)\n","  adding: model/frame21.jpg (deflated 6%)\n","  adding: model/frame64.jpg (deflated 6%)\n","  adding: model/frame132.jpg (deflated 6%)\n","  adding: model/frame33.jpg (deflated 5%)\n","  adding: model/frame14.jpg (deflated 6%)\n","  adding: model/frame121.jpg (deflated 5%)\n","  adding: model/frame56.jpg (deflated 5%)\n","  adding: model/frame98.jpg (deflated 5%)\n","  adding: model/frame46.jpg (deflated 6%)\n","  adding: model/frame91.jpg (deflated 6%)\n","  adding: model/frame48.jpg (deflated 6%)\n","  adding: model/frame35.jpg (deflated 6%)\n","  adding: model/frame101.jpg (deflated 5%)\n","  adding: model/frame53.jpg (deflated 6%)\n","  adding: model/frame111.jpg (deflated 6%)\n","  adding: model/frame3.jpg (deflated 5%)\n","  adding: model/frame8.jpg (deflated 5%)\n","  adding: model/frame102.jpg (deflated 5%)\n","  adding: model/frame40.jpg (deflated 6%)\n","  adding: model/frame9.jpg (deflated 6%)\n","  adding: model/frame30.jpg (deflated 6%)\n","  adding: model/frame110.jpg (deflated 7%)\n","  adding: model/frame99.jpg (deflated 5%)\n","  adding: model/frame112.jpg (deflated 6%)\n","  adding: model/frame131.jpg (deflated 6%)\n","  adding: model/frame143.jpg (deflated 6%)\n","  adding: model/frame84.jpg (deflated 7%)\n","  adding: model/frame124.jpg (deflated 4%)\n","  adding: model/frame61.jpg (deflated 6%)\n","  adding: model/frame10.jpg (deflated 6%)\n","  adding: model/frame74.jpg (deflated 6%)\n","  adding: model/frame15.jpg (deflated 6%)\n","  adding: model/frame27.jpg (deflated 5%)\n","  adding: model/frame39.jpg (deflated 6%)\n","  adding: model/frame133.jpg (deflated 8%)\n","  adding: model/frame136.jpg (deflated 6%)\n","  adding: model/frame125.jpg (deflated 5%)\n","  adding: model/frame16.jpg (deflated 6%)\n","  adding: model/frame147.jpg (deflated 5%)\n","  adding: model/frame86.jpg (deflated 7%)\n","  adding: model/frame18.jpg (deflated 5%)\n","  adding: model/frame65.jpg (deflated 6%)\n","  adding: model/frame144.jpg (deflated 4%)\n","  adding: model/frame50.jpg (deflated 7%)\n","  adding: model/frame13.jpg (deflated 6%)\n","  adding: model/frame87.jpg (deflated 7%)\n","  adding: model/frame113.jpg (deflated 6%)\n","  adding: model/frame137.jpg (deflated 8%)\n","  adding: model/frame114.jpg (deflated 6%)\n","  adding: model/frame11.jpg (deflated 6%)\n","  adding: model/frame130.jpg (deflated 6%)\n","  adding: model/frame105.jpg (deflated 6%)\n","  adding: model/frame89.jpg (deflated 6%)\n","  adding: model/frame17.jpg (deflated 5%)\n","  adding: model/frame123.jpg (deflated 5%)\n","  adding: model/frame26.jpg (deflated 5%)\n","  adding: model/frame60.jpg (deflated 6%)\n","  adding: model/frame19.jpg (deflated 5%)\n","  adding: model/frame118.jpg (deflated 6%)\n","  adding: model/frame4.jpg (deflated 5%)\n"]}]},{"cell_type":"code","source":["!zip -r mp4tojpg_person.zip person/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ljhAhMVouWEP","executionInfo":{"status":"ok","timestamp":1671019972904,"user_tz":-300,"elapsed":762,"user":{"displayName":"Людмила Рябкова","userId":"09418750656736874784"}},"outputId":"eee1c1b2-c132-4eef-dceb-48652cb46f15"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["  adding: person/ (stored 0%)\n","  adding: person/frame139.jpg (deflated 0%)\n","  adding: person/frame67.jpg (deflated 0%)\n","  adding: person/frame129.jpg (deflated 0%)\n","  adding: person/frame97.jpg (deflated 0%)\n","  adding: person/frame36.jpg (deflated 0%)\n","  adding: person/frame69.jpg (deflated 0%)\n","  adding: person/frame76.jpg (deflated 0%)\n","  adding: person/frame100.jpg (deflated 0%)\n","  adding: person/frame12.jpg (deflated 0%)\n","  adding: person/frame5.jpg (deflated 0%)\n","  adding: person/frame104.jpg (deflated 0%)\n","  adding: person/frame145.jpg (deflated 0%)\n","  adding: person/frame122.jpg (deflated 0%)\n","  adding: person/frame120.jpg (deflated 0%)\n","  adding: person/frame82.jpg (deflated 0%)\n","  adding: person/frame45.jpg (deflated 0%)\n","  adding: person/frame1.jpg (deflated 0%)\n","  adding: person/frame6.jpg (deflated 0%)\n","  adding: person/frame54.jpg (deflated 0%)\n","  adding: person/frame83.jpg (deflated 0%)\n","  adding: person/frame42.jpg (deflated 0%)\n","  adding: person/frame51.jpg (deflated 0%)\n","  adding: person/frame134.jpg (deflated 0%)\n","  adding: person/frame34.jpg (deflated 0%)\n","  adding: person/frame127.jpg (deflated 0%)\n","  adding: person/frame23.jpg (deflated 0%)\n","  adding: person/frame78.jpg (deflated 0%)\n","  adding: person/frame92.jpg (deflated 0%)\n","  adding: person/frame2.jpg (deflated 0%)\n","  adding: person/frame37.jpg (deflated 0%)\n","  adding: person/frame109.jpg (deflated 0%)\n","  adding: person/frame41.jpg (deflated 0%)\n","  adding: person/frame0.jpg (deflated 0%)\n","  adding: person/frame52.jpg (deflated 0%)\n","  adding: person/frame49.jpg (deflated 0%)\n","  adding: person/frame43.jpg (deflated 0%)\n","  adding: person/frame75.jpg (deflated 0%)\n","  adding: person/frame22.jpg (deflated 0%)\n","  adding: person/frame63.jpg (deflated 0%)\n","  adding: person/frame141.jpg (deflated 0%)\n","  adding: person/frame57.jpg (deflated 0%)\n","  adding: person/frame119.jpg (deflated 0%)\n","  adding: person/frame116.jpg (deflated 0%)\n","  adding: person/frame62.jpg (deflated 0%)\n","  adding: person/frame68.jpg (deflated 0%)\n","  adding: person/frame70.jpg (deflated 0%)\n","  adding: person/frame135.jpg (deflated 0%)\n","  adding: person/frame103.jpg (deflated 0%)\n","  adding: person/frame96.jpg (deflated 0%)\n","  adding: person/frame85.jpg (deflated 0%)\n","  adding: person/frame7.jpg (deflated 0%)\n","  adding: person/frame142.jpg (deflated 0%)\n","  adding: person/frame115.jpg (deflated 0%)\n","  adding: person/frame59.jpg (deflated 0%)\n","  adding: person/frame29.jpg (deflated 0%)\n","  adding: person/frame107.jpg (deflated 0%)\n","  adding: person/frame79.jpg (deflated 0%)\n","  adding: person/frame126.jpg (deflated 0%)\n","  adding: person/frame31.jpg (deflated 0%)\n","  adding: person/frame77.jpg (deflated 0%)\n","  adding: person/frame117.jpg (deflated 0%)\n","  adding: person/frame20.jpg (deflated 0%)\n","  adding: person/frame146.jpg (deflated 0%)\n","  adding: person/frame88.jpg (deflated 0%)\n","  adding: person/frame32.jpg (deflated 0%)\n","  adding: person/frame128.jpg (deflated 0%)\n","  adding: person/frame108.jpg (deflated 0%)\n","  adding: person/frame28.jpg (deflated 0%)\n","  adding: person/frame140.jpg (deflated 0%)\n","  adding: person/frame24.jpg (deflated 0%)\n","  adding: person/frame106.jpg (deflated 0%)\n","  adding: person/frame80.jpg (deflated 0%)\n","  adding: person/frame138.jpg (deflated 0%)\n","  adding: person/frame66.jpg (deflated 0%)\n","  adding: person/frame47.jpg (deflated 0%)\n","  adding: person/frame93.jpg (deflated 0%)\n","  adding: person/frame71.jpg (deflated 0%)\n","  adding: person/frame81.jpg (deflated 0%)\n","  adding: person/frame55.jpg (deflated 0%)\n","  adding: person/frame44.jpg (deflated 0%)\n","  adding: person/frame73.jpg (deflated 0%)\n","  adding: person/frame25.jpg (deflated 0%)\n","  adding: person/frame58.jpg (deflated 0%)\n","  adding: person/frame95.jpg (deflated 0%)\n","  adding: person/frame38.jpg (deflated 0%)\n","  adding: person/frame94.jpg (deflated 0%)\n","  adding: person/frame72.jpg (deflated 0%)\n","  adding: person/frame90.jpg (deflated 0%)\n","  adding: person/frame21.jpg (deflated 0%)\n","  adding: person/frame64.jpg (deflated 0%)\n","  adding: person/frame132.jpg (deflated 0%)\n","  adding: person/frame33.jpg (deflated 0%)\n","  adding: person/frame14.jpg (deflated 0%)\n","  adding: person/frame121.jpg (deflated 0%)\n","  adding: person/frame56.jpg (deflated 0%)\n","  adding: person/frame98.jpg (deflated 0%)\n","  adding: person/frame46.jpg (deflated 0%)\n","  adding: person/frame91.jpg (deflated 0%)\n","  adding: person/frame48.jpg (deflated 0%)\n","  adding: person/frame35.jpg (deflated 0%)\n","  adding: person/frame101.jpg (deflated 0%)\n","  adding: person/frame53.jpg (deflated 0%)\n","  adding: person/frame111.jpg (deflated 0%)\n","  adding: person/frame3.jpg (deflated 0%)\n","  adding: person/frame8.jpg (deflated 0%)\n","  adding: person/frame102.jpg (deflated 0%)\n","  adding: person/frame40.jpg (deflated 0%)\n","  adding: person/frame9.jpg (deflated 0%)\n","  adding: person/frame30.jpg (deflated 0%)\n","  adding: person/frame110.jpg (deflated 0%)\n","  adding: person/frame99.jpg (deflated 0%)\n","  adding: person/frame112.jpg (deflated 0%)\n","  adding: person/frame131.jpg (deflated 0%)\n","  adding: person/frame143.jpg (deflated 0%)\n","  adding: person/frame84.jpg (deflated 0%)\n","  adding: person/frame124.jpg (deflated 0%)\n","  adding: person/frame61.jpg (deflated 0%)\n","  adding: person/frame10.jpg (deflated 0%)\n","  adding: person/frame74.jpg (deflated 0%)\n","  adding: person/frame15.jpg (deflated 0%)\n","  adding: person/frame27.jpg (deflated 0%)\n","  adding: person/frame39.jpg (deflated 0%)\n","  adding: person/frame133.jpg (deflated 0%)\n","  adding: person/frame136.jpg (deflated 0%)\n","  adding: person/frame125.jpg (deflated 0%)\n","  adding: person/frame16.jpg (deflated 0%)\n","  adding: person/frame147.jpg (deflated 0%)\n","  adding: person/frame86.jpg (deflated 0%)\n","  adding: person/frame18.jpg (deflated 0%)\n","  adding: person/frame65.jpg (deflated 0%)\n","  adding: person/frame144.jpg (deflated 0%)\n","  adding: person/frame50.jpg (deflated 0%)\n","  adding: person/frame13.jpg (deflated 0%)\n","  adding: person/frame87.jpg (deflated 0%)\n","  adding: person/frame113.jpg (deflated 0%)\n","  adding: person/frame137.jpg (deflated 0%)\n","  adding: person/frame114.jpg (deflated 0%)\n","  adding: person/frame11.jpg (deflated 0%)\n","  adding: person/frame130.jpg (deflated 0%)\n","  adding: person/frame105.jpg (deflated 0%)\n","  adding: person/frame89.jpg (deflated 0%)\n","  adding: person/frame17.jpg (deflated 0%)\n","  adding: person/frame123.jpg (deflated 0%)\n","  adding: person/frame26.jpg (deflated 0%)\n","  adding: person/frame60.jpg (deflated 0%)\n","  adding: person/frame19.jpg (deflated 0%)\n","  adding: person/frame118.jpg (deflated 0%)\n","  adding: person/frame4.jpg (deflated 0%)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"2BbMrF1innUD"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOLgVuWmDH6numsoMO4w+TP"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"7698f359a10c4684b7edf60eff61dfef":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5773d1092cd94bfeab65697dccddae50","IPY_MODEL_1e0aab20c17b4d3a8ce8a34aab977b3f","IPY_MODEL_6bbf2c5c87d14ebb98e7e03960ad0c88"],"layout":"IPY_MODEL_48bf454773664ec789069fcbd13c542b"}},"5773d1092cd94bfeab65697dccddae50":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e83f0dac5262415b88bf93ead68ae9ab","placeholder":"​","style":"IPY_MODEL_db2a8569dfa64d109ea9dd627e321414","value":"100%"}},"1e0aab20c17b4d3a8ce8a34aab977b3f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a4be9f35b9b44955a4412ea0cb333190","max":237034793,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a85356ca0799433cb09629903e0401cd","value":237034793}},"6bbf2c5c87d14ebb98e7e03960ad0c88":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_95ea2b0d2e6246afa2c5ed12a96cc388","placeholder":"​","style":"IPY_MODEL_def102ffbf234940a2d8ef65f2e3c4c9","value":" 226M/226M [00:04&lt;00:00, 35.5MB/s]"}},"48bf454773664ec789069fcbd13c542b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e83f0dac5262415b88bf93ead68ae9ab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db2a8569dfa64d109ea9dd627e321414":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a4be9f35b9b44955a4412ea0cb333190":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a85356ca0799433cb09629903e0401cd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"95ea2b0d2e6246afa2c5ed12a96cc388":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"def102ffbf234940a2d8ef65f2e3c4c9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}